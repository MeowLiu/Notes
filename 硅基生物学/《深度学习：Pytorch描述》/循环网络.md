# 循环伸神经网络

## 1.1对序列进行建模

### 1.1.1问题提出

在进行建模的时候，常是已知前 $t-1$ 个数据来预测第 $t$ 个数据，用条件概率表示如下。
$$
x_t \sim~ P(x_t | x_1, x_2,...,x_{t-1})
$$
想要预测第 $t$ 个数据就必须知道前 $t-1$ 个数据与第 $t$ 个数之间的关系，所以用函数进行建模，统筹联系。
$$
P(x_t | x_1, x_2,...,x_{t-1}) =P(x_t | f(x_1, x_2,...,x_{t-1}))
$$
在深度学习中 $f(x_1, x_2,...,x_{t-1})$ 就是我训练的模型。

### 1.1.2解决方案的讨论

#### 1.1.2.1问题阐述

要进行建模需要解决两个问题：

- 如何计算 $f(x_1, x_2,...,x_{t-1}))$ 
- 如何计算 $x_t$

#### 1.1.2.2提出方案

方案一：**马尔科夫假设**。

利用马尔可夫模型，只取前 $\tau$ 个数据样本来进行预测

方案一：**隐变量自回归**。

利用隐变量 $h_{t}$ 来对 $t$ 时刻之前的数据进行总结，再利用隐变量进行预测。
$$
\hat{x}_t = P(x_t|h_t)
$$
其中隐变量 $h_t$ 与 $x_{t-1}$ 和 $h_{t-1}$ 有关
$$
h_t = g(h_{t-1},x_{t-1})
$$

## 1.2语言模型

### 1.2.1语言模型的目标

语言模型的目标就是计文本序列的联合概率，比如要计算 $P(deep, learning)$ ，用条件概率公式计算就是$P(deep, learning) = P(deep) \cdot P(learning|deep)$，这是预测连续出现的两个单词的概率，在更复杂的语言模型中，要求预测更长的序列，但这变得更加困难。

### 1.2.2马尔可夫模型与n元语法

对于一个预测任务，如果将预测值之前的所有数据都进行计算将会产生巨大的时间复杂度，因此用马尔可夫模型来减少用于计算的序列长度是一个好的选择。